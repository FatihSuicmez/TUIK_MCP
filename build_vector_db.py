import os
import json
import pandas as pd
import google.generativeai as genai
from io import StringIO
import time
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import pickle
from multiprocessing import Pool, cpu_count, freeze_support
import argparse
import csv
from datetime import datetime

# --- KONTROL NOKTASI VE LOG DOSYA ADLARI ---
PROCESSED_LOG_FILE = 'processed_files.log'
FAILED_LOG_FILE = 'failed_files.log'
CHUNKS_CHECKPOINT_FILE = 'all_chunks.pkl'

# ==============================================================================
# FONKSÄ°YON 1: TÃ¼m Dosya Bilgilerini YÃ¼kleme
# ==============================================================================
def load_all_files_from_data_json():
    """
    data.json dosyasÄ±nÄ± okur ve her dosya iÃ§in tam yolunu ve kategori adÄ±nÄ±
    iÃ§eren bir liste dÃ¶ndÃ¼rÃ¼r.
    """
    print("data.json okunuyor ve dosya yollarÄ± hazÄ±rlanÄ±yor...")
    try:
        with open('data.json', 'r', encoding='utf-8') as f:
            all_data = json.load(f)
    except FileNotFoundError:
        print("HATA: data.json dosyasÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce prepare_data.py betiÄŸini Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return []

    file_info_list = []
    base_data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')
    for category in all_data:
        kategori_folder = category.get("kategori")
        kategori_adi = category.get("name", "Bilinmeyen Kategori")
        files = category.get("files", [])
        for file_name in files:
            full_path = os.path.join(base_data_dir, kategori_folder, file_name)
            if os.path.exists(full_path):
                file_info_list.append({
                    "path": full_path,
                    "category": kategori_adi
                })
    print(f"Toplam {len(file_info_list)} adet Excel dosyasÄ± bulundu.")
    return file_info_list

# ==============================================================================
# FONKSÄ°YON 2: AyrÄ±ntÄ±lÄ± Hata KaydÄ±
# ==============================================================================
def log_failure(file_info, error_message):
    """BaÅŸarÄ±sÄ±z olan dosyalarÄ± ve hata nedenlerini CSV dosyasÄ±na yazar."""
    file_basename = os.path.basename(file_info['path'])
    category_name = file_info['category']
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    write_header = not os.path.exists(FAILED_LOG_FILE)
    with open(FAILED_LOG_FILE, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(['timestamp', 'category', 'filename', 'error_message'])
        writer.writerow([timestamp, category_name, file_basename, str(error_message)])

# ==============================================================================
# FONKSÄ°YON 3: Gemini ile Chunk OluÅŸturma (Tekrar Deneme MekanizmalÄ±)
# ==============================================================================
def get_llm_chunks_from_gemini(table_as_csv_string: str, file_name: str, max_retries: int = 3) -> list:
    model = genai.GenerativeModel('gemini-2.5-pro')
    prompt = f"""
    Sen, karmaÅŸÄ±k ve dÃ¼zensiz TÃœÄ°K Excel tablolarÄ±nÄ± analiz etme konusunda uzman bir veri analistisin.
    GÃ¶revin, sana CSV formatÄ±nda verilen bir tabloyu inceleyip, iÃ§indeki her anlamlÄ± veri noktasÄ±nÄ±,
    kendi baÅŸÄ±na bir anlam ifade eden, baÄŸlamÄ± zenginleÅŸtirilmiÅŸ tam bir cÃ¼mleye dÃ¶nÃ¼ÅŸtÃ¼rmektir.
    - Sadece gerÃ§ek veri iÃ§eren satÄ±rlara odaklan.
    - Sonucu, her cÃ¼mlenin bir eleman olduÄŸu bir JSON Array (liste) olarak dÃ¶ndÃ¼r.
    - Sadece ve sadece JSON listesini dÃ¶ndÃ¼r, baÅŸka hiÃ§bir aÃ§Ä±klama veya metin ekleme.
    Ä°ÅŸte analiz edilecek tablo. Dosya AdÄ±: {file_name}\n\nTablo (CSV FormatÄ±):\n{table_as_csv_string}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(temperature=0.0))
            if not response.parts:
                if response.prompt_feedback.block_reason:
                    raise Exception(f"Cevap gÃ¼venlik nedeniyle engellendi (Sebep: {response.prompt_feedback.block_reason.name})")
                else:
                    raise Exception("Model boÅŸ bir cevap dÃ¶ndÃ¼rdÃ¼.")
            content = response.text
            if content.strip().startswith("```json"):
                content = content.strip()[7:-3].strip()
            generated_chunks = json.loads(content)
            final_chunks = []
            if isinstance(generated_chunks, list):
                for text in generated_chunks:
                    final_chunks.append({'text': str(text), 'metadata': {'source': file_name, 'type': 'llm_generated_data_point'}})
            return final_chunks
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** (attempt + 1)
                print(f"     âš ï¸ Hata (Deneme {attempt + 1}/{max_retries}), {wait_time} saniye sonra tekrar denenecek: {e}")
                time.sleep(wait_time)
            else:
                raise e
    return []

# ==============================================================================
# FONKSÄ°YON 4: Tek DosyayÄ± Ä°ÅŸleme
# ==============================================================================
def process_file_with_llm(file_path):
    try:
        df = pd.read_excel(file_path, header=None, engine='openpyxl' if file_path.endswith('.xlsx') else 'xlrd')
        df.dropna(how='all', inplace=True); df.dropna(how='all', axis=1, inplace=True)
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, header=False)
        csv_string = csv_buffer.getvalue()
        if len(csv_string.splitlines()) > 250:
            csv_string = "\n".join(csv_string.splitlines()[:250])
        return get_llm_chunks_from_gemini(csv_string, os.path.basename(file_path))
    except Exception as e:
        raise e

# ==============================================================================
# FONKSÄ°YON 5: Multiprocessing iÃ§in SarmalayÄ±cÄ± Fonksiyon
# ==============================================================================
def process_file_wrapper(args):
    index, total, file_info = args
    file_basename = os.path.basename(file_info['path'])
    try:
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key: raise ValueError("GOOGLE_API_KEY worker process'te bulunamadÄ±.")
        genai.configure(api_key=api_key)
        print(f"  -> [{index}/{total} | {file_info['category']}] Ä°ÅŸleniyor: {file_basename}")
        result_chunks = process_file_with_llm(file_info['path'])
        return (file_basename, result_chunks)
    except Exception as e:
        log_failure(file_info, e)
        return (file_basename, [])

# ==============================================================================
# FONKSÄ°YON 6: Ana Fonksiyon
# ==============================================================================
def main():
    parser = argparse.ArgumentParser(description="TÃœÄ°K verilerini iÅŸleyip RAG veritabanÄ± oluÅŸturan betik.")
    parser.add_argument('--reprocess-failed', action='store_true', help="Sadece 'failed_files.log' dosyasÄ±ndaki baÅŸarÄ±sÄ±z dosyalarÄ± yeniden iÅŸler.")
    args = parser.parse_args()
    print(f"--- RAG VeritabanÄ± OluÅŸturucu BaÅŸlatÄ±ldÄ± ---")
    
    try:
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key: raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±.")
        genai.configure(api_key=api_key)
        print("âœ… Google API anahtarÄ± baÅŸarÄ±yla yÃ¼klendi.")
    except Exception as e:
        print(f"âŒ HATA: {e}"); exit()

    full_file_info_list = load_all_files_from_data_json()
    if not full_file_info_list: exit()

    files_to_process_info = []
    if args.reprocess_failed:
        print(f"** Yeniden Ä°ÅŸleme Modu Aktif **")
        if not os.path.exists(FAILED_LOG_FILE): print(f"'{FAILED_LOG_FILE}' bulunamadÄ±."); exit()
        with open(FAILED_LOG_FILE, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            try:
                next(reader); failed_filenames = {row[2] for row in reader}
            except StopIteration: failed_filenames = set()
        if not failed_filenames: print("BaÅŸarÄ±sÄ±z dosya bulunamadÄ±."); exit()
        print(f"'{FAILED_LOG_FILE}' dosyasÄ±ndan {len(failed_filenames)} adet baÅŸarÄ±sÄ±z dosya yeniden iÅŸlenecek.")
        files_to_process_info = [info for info in full_file_info_list if os.path.basename(info['path']) in failed_filenames]
        os.remove(FAILED_LOG_FILE)
    else:
        print("** Normal Ä°ÅŸleme Modu Aktif **")
        processed_files = set()
        if os.path.exists(PROCESSED_LOG_FILE):
            with open(PROCESSED_LOG_FILE, 'r', encoding='utf-8') as f:
                processed_files = set(line.strip() for line in f)
        files_to_process_info = [info for info in full_file_info_list if os.path.basename(info['path']) not in processed_files]

    if not files_to_process_info:
        print("âœ… Ä°ÅŸlenecek yeni dosya bulunamadÄ±. GÃ¶mme (embedding) adÄ±mÄ±na geÃ§iliyor.")
    else:
        print(f"\n{len(files_to_process_info)} adet dosya Gemini ile iÅŸlenecek (paralel olarak)...")
        worker_count = 8
        print(f"KullanÄ±lacak paralel iÅŸlemci sayÄ±sÄ±: {worker_count}")
        tasks_with_metadata = [(idx + 1, len(files_to_process_info), info) for idx, info in enumerate(files_to_process_info)]
        
        with Pool(processes=worker_count) as pool:
            for file_basename, result_chunks in pool.imap_unordered(process_file_wrapper, tasks_with_metadata):
                if result_chunks:
                    print(f"     ... {file_basename} iÃ§in {len(result_chunks)} adet chunk baÅŸarÄ±yla oluÅŸturuldu.")
                    with open(PROCESSED_LOG_FILE, 'a', encoding='utf-8') as f_log:
                        f_log.write(f"{file_basename}\n")
                    with open(CHUNKS_CHECKPOINT_FILE, 'ab') as f_chunks:
                        pickle.dump(result_chunks, f_chunks)

    print("\nParalel iÅŸlemler tamamlandÄ±. SonuÃ§lar birleÅŸtiriliyor...")
    all_chunks = []
    if os.path.exists(CHUNKS_CHECKPOINT_FILE):
        try:
            file_size_mb = os.path.getsize(CHUNKS_CHECKPOINT_FILE) / (1024 * 1024)
            print(f"'{CHUNKS_CHECKPOINT_FILE}' okunuyor ({file_size_mb:.2f} MB). Bu iÅŸlem biraz sÃ¼rebilir...")
            with open(CHUNKS_CHECKPOINT_FILE, 'rb') as f:
                while True:
                    all_chunks.extend(pickle.load(f))
        except EOFError:
            pass
        except Exception as e:
            print(f"Checkpoint dosyasÄ± okunurken hata: {e}")
    
    if not all_chunks:
        print("âŒ HiÃ§ metin parÃ§asÄ± (chunk) oluÅŸturulamadÄ±. GÃ¶mme iÅŸlemi atlanÄ±yor."); exit()
    print(f"âœ… HafÄ±zaya toplam {len(all_chunks)} adet chunk yÃ¼klendi.")
    
    model_name = 'paraphrase-multilingual-mpnet-base-v2'
    print(f"\nEmbedding modeli ('{model_name}') yÃ¼kleniyor...")
    print("Not: Bu model ~1.11 GB boyutundadÄ±r ve hafÄ±zaya yÃ¼klenmesi zaman alabilir.")
    model = SentenceTransformer(model_name)
    print("âœ… Embedding modeli baÅŸarÄ±yla yÃ¼klendi.")
    
    texts_to_embed = [chunk['text'] for chunk in all_chunks]
    print(f"\n{len(texts_to_embed)} adet metin parÃ§asÄ± vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    embeddings = model.encode(texts_to_embed, show_progress_bar=True)
    embeddings = np.array(embeddings).astype('float32')
    
    embedding_dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(embedding_dimension)
    index.add(embeddings)
    faiss.write_index(index, 'tuik_faiss.index')
    print(f"âœ… FAISS veritabanÄ± 'tuik_faiss.index' olarak kaydedildi.")

    with open('tuik_chunks.pkl', 'wb') as f:
        pickle.dump(all_chunks, f)
    print("âœ… Metin parÃ§alarÄ± kalÄ±cÄ± olarak 'tuik_chunks.pkl' dosyasÄ±na kaydedildi.")
    print("\nğŸ‰ Tebrikler! RAG veritabanÄ±nÄ±z baÅŸarÄ±yla oluÅŸturuldu! ğŸ‰")

# ==============================================================================
# Betik BaÅŸlatma NoktasÄ±
# ==============================================================================
if __name__ == "__main__":
    freeze_support()
    main()